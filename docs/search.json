[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bayesian Unobserved Component Models",
    "section": "",
    "text": "Abstract. We present the basics of Bayesian estimation and inference for unobserved component models on the example of a local-level model. The range of topics includes the conjugate prior analysis using normal-inverted-gamma 2 distribution and its extensions focusing on hierarchical modelling, conditional heteroskedasticity, and Student-t error terms. We scrutinise Bayesian forecasting and sampling from the predictive density.\nKeywords. Unobserved Component Models, Local-Level Model, State-Space Bayesian Inference, Forecasting, Heteroskedasticity, Hierarchical Modelling, Gibbs Sampler, Simulation Smoother, Precision Sampling"
  },
  {
    "objectID": "index.html#matrix-notation-for-the-model",
    "href": "index.html#matrix-notation-for-the-model",
    "title": "Bayesian Unobserved Component Models",
    "section": "Matrix notation for the model",
    "text": "Matrix notation for the model\nTo simplify the notation and the derivations introduce matrix notation for the model. Let \\(T\\) be the available sample size for the variable \\(y\\). Define a \\(T\\)-vector of zeros, \\(\\mathbf{0}_T\\), and of ones, \\(\\boldsymbol\\imath_T\\), the identity matrix of order \\(T\\), \\(\\mathbf{I}_T\\), as well as \\(T\\times1\\) vectors: \\[\\begin{align}\n\\mathbf{y} = \\begin{bmatrix} y_1\\\\ \\vdots\\\\ y_T \\end{bmatrix},\\quad\n\\boldsymbol\\tau = \\begin{bmatrix} \\tau_1\\\\ \\vdots\\\\ \\tau_T \\end{bmatrix},\\quad\n\\boldsymbol\\epsilon = \\begin{bmatrix} \\epsilon_1\\\\ \\vdots\\\\ \\epsilon_T \\end{bmatrix},\\quad\n\\boldsymbol\\eta = \\begin{bmatrix} \\eta_1\\\\ \\vdots\\\\ \\eta_T \\end{bmatrix},\\qquad\n\\mathbf{i} = \\begin{bmatrix} 1\\\\0\\\\ \\vdots\\\\ 0 \\end{bmatrix},\n\\end{align}\\] and a \\(T\\times T\\) matrix \\(\\mathbf{H}\\) with the elements: \\[\\begin{align}\n\\mathbf{H} = \\begin{bmatrix}\n1 & 0 & \\cdots & 0 & 0\\\\\n-1 & 1 & \\cdots & 0 & 0\\\\\n0 & -1 & \\cdots & 0 & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\n0 & 0 & \\cdots & 1 & 0\\\\\n0 & 0 & \\cdots & -1 & 1\n\\end{bmatrix}.\n\\end{align}\\]\nThen the model can be written in a concise notation as: \\[\\begin{align}\n\\mathbf{y} &= \\mathbf{\\tau} + \\boldsymbol\\epsilon,\\\\\n\\mathbf{H}\\boldsymbol\\tau &= \\mathbf{i} \\tau_0 + \\boldsymbol\\eta,\\\\\n\\boldsymbol\\epsilon &\\sim\\mathcal{N}\\left(\\mathbf{0}_T, \\sigma^2\\mathbf{I}_T\\right),\\\\\n\\boldsymbol\\eta &\\sim\\mathcal{N}\\left(\\mathbf{0}_T, \\sigma_\\eta^2\\mathbf{I}_T\\right).\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#likelihood-function",
    "href": "index.html#likelihood-function",
    "title": "Bayesian Unobserved Component Models",
    "section": "Likelihood function",
    "text": "Likelihood function\nThe model equations imply the predictive density of the data vector \\(\\mathbf{y}\\). To see this, consider the model equation as a linear transformation of a normal vector \\(\\boldsymbol\\epsilon\\). Therefore, the data vector follows a multivariate normal distribution given by: \\[\\begin{align}\n\\mathbf{y}\\mid \\boldsymbol\\tau, \\sigma^2 &\\sim\\mathcal{N}_T\\left(\\boldsymbol\\tau, \\sigma^2\\mathbf{I}_T\\right).\n\\end{align}\\]\nThis distribution determines the shape of the likelihood function that is defined as the sampling data density: \\[\\begin{align}\nL(\\boldsymbol\\tau,\\sigma^2|\\mathbf{y})\\equiv p\\left(\\mathbf{y}\\mid\\boldsymbol\\tau, \\sigma^2 \\right).\n\\end{align}\\]\nThe likelihood function that for the sake of the estimation of the parameters, and after plugging in data in place of \\(\\mathbf{y}\\), is considered a function of parameters \\(\\boldsymbol\\tau\\) and \\(\\sigma^2\\) is given by: \\[\\begin{align}\nL(\\boldsymbol\\tau,\\sigma^2|\\mathbf{y}) =\n(2\\pi)^{-\\frac{T}{2}}\\left(\\sigma^2\\right)^{-\\frac{T}{2}}\\exp\\left\\{-\\frac{1}{2}\\frac{1}{\\sigma^2}(\\mathbf{y} - \\boldsymbol\\tau)'(\\mathbf{y} - \\boldsymbol\\tau)\\right\\}.\n\\end{align}\\]"
  },
  {
    "objectID": "index.html#prior-distributions",
    "href": "index.html#prior-distributions",
    "title": "Bayesian Unobserved Component Models",
    "section": "Prior distributions",
    "text": "Prior distributions\nThe state equation for \\(\\boldsymbol\\tau\\) can be rewritten as follows: \\[\n\\begin{gather}\n\\boldsymbol\\tau = \\mathbf{H}^{-1} \\mathbf{i} \\tau_0+\\mathbf{H}^{-1}\\boldsymbol\\eta \\\\\n\\\\ \\boldsymbol\\eta \\sim \\mathcal{N}(\\mathbf{0}_T, \\sigma_\\eta^2 \\mathbf{I}_T) \\\\\n\\mathbf{H}^{-1} \\boldsymbol\\eta \\sim \\mathcal{N}(\\mathbf{0}_T, \\sigma_\\eta^2\\left(\\mathbf{H}^{\\prime} \\mathbf{H}\\right)^{-1})\n\\end{gather}\n\\] Using the state equation for \\(\\boldsymbol\\tau\\) above, we can derive the prior distribution of \\(\\boldsymbol\\tau\\) as: \\[\n\\begin{align}\n\\boldsymbol\\tau | \\tau_0, \\sigma_\\eta^2 &\\sim \\mathcal{N}_T(\\mathbf{H}^{-1} \\mathbf{i} \\tau_0, \\sigma_\\eta^2(\\mathbf{H}^{\\prime} \\mathbf{H})^{-1})\n\\\\ &\\propto \\exp \\left\\{-\\frac{1}{2} \\frac{1}{\\sigma_\\eta^2}\\left(\\boldsymbol\\tau-\\mathbf{H}^{-1} \\mathbf{i} \\tau_0\\right)^{\\prime}\\left(\\mathbf{H}^{\\prime} \\mathbf{H}\\right)\\left(\\boldsymbol\\tau-\\mathbf{H}^{-1} \\mathbf{i} \\tau_0\\right)\\right\\}\n\\end{align}\n\\] where the prior distribution of \\(\\tau_0\\) is derived as follows: \\[\n\\begin{align}\n\\tau_0 &\\sim \\mathcal{N}(\\underline{\\tau_0}, \\underline{V_{\\tau_{0}}})\n\\\\ &\\propto \\exp \\left\\{-\\frac{1}{2}(\\tau_0-\\underline{\\tau_0})^{\\prime} \\underline{V_{\\tau_{0}}}^{-1}(\\tau_0-\\underline{\\tau_0})\\right\\}\n\\end{align}\n\\] and the prior distribution of \\(\\sigma_\\eta^2\\) is the following: \\[\n\\begin{align}\n\\sigma_\\eta^2 &\\sim \\mathcal{IG}2(\\underline{s_\\eta}, \\underline{v_\\eta}) \\\\\n&\\propto (\\sigma_\\eta^2)^{-\\frac{\\underline{v}+2}{2}} \\exp \\left\\{-\\frac{1}{2} \\frac{s}{\\sigma_\\eta^2}\\right\\}\n\\end{align}\n\\]\nGiven that \\(\\boldsymbol\\epsilon\\) follows normal distribution, \\[\n\\begin{align}\n\\boldsymbol\\epsilon &\\sim \\mathcal{N}(\\mathbf{0}_T, \\sigma^2 \\mathbf{I}_T)\n\\end{align}\n\\] the prior distribution of \\(\\boldsymbol\\epsilon\\) given \\(\\sigma^2\\) is the following: \\[\n\\begin{align}\n\\boldsymbol\\epsilon | \\sigma^2 &\\sim \\mathcal{N}(\\mathbf{0}_T, \\sigma^2 \\mathbf{I}_T)\n\\\\ &\\propto \\exp \\left\\{-\\frac{1}{2} \\frac{1}{\\sigma^2} \\boldsymbol\\epsilon^{\\prime}\\boldsymbol\\epsilon\\right\\}\n\\end{align}\n\\]\nThe prior distribution of \\(\\sigma^2\\) is the following: \\[\n\\begin{align}\n\\sigma^2 &\\sim \\mathcal{IG}2(\\underline{s}, \\underline{v}) \\\\\n&\\propto (\\sigma^2)^{-\\frac{\\underline{v}+2}{2}} \\exp \\left\\{-\\frac{1}{2} \\frac{s}{\\sigma^2}\\right\\}\n\\end{align}\n\\]\nThe Joint prior distribution of \\(\\boldsymbol\\tau\\), \\(\\tau_0\\), \\(\\sigma^2_{\\eta}\\), and \\(\\sigma^2\\) is then derived as follows: \\[\n\\begin{aligned}\np(\\boldsymbol\\tau,\\tau_{0},\\sigma_{\\eta}^{2},\\sigma^{2}) = p(\\boldsymbol\\tau|\\tau_{0},\\sigma_{\\eta}^{2}) \\space p(\\tau_{0}) \\space p(\\sigma_{\\eta}^{2}) \\space p(\\sigma^{2})\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "index.html#gibbs-sampler",
    "href": "index.html#gibbs-sampler",
    "title": "Bayesian Unobserved Component Models",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\nA Gibbs Sampler can be applied using the following steps:\nAt each iteration \\(s\\) where \\(s\\) goes from 1 to \\(S\\),\n   Step 1. Draw \\(\\tau_0^{(s)}\\) from the \\(N({\\tau_{0}}, {V_{\\tau_{0}}})\\) distribution and collect \\(\\tau_0\\).\n   Step 2. Draw \\({\\sigma^2_{\\eta}}^{(s)}\\) from the \\(\\mathcal{IG}2({S_\\eta}, {v_\\eta})\\) distribution and collect \\(\\sigma_\\eta^2\\).\n   Step 3. Draw \\({\\sigma^2}^{(s)}\\) from the \\(\\mathcal{IG}2(S, v)\\) distribution and collect \\(\\sigma^2\\).\n   Step 4. Draw \\(\\boldsymbol\\tau^{(s)}\\) from the \\(\\mathcal{N}({\\boldsymbol\\tau},{V_{\\tau}})\\) distribution and collect \\(\\boldsymbol\\tau\\).\n\n# Gibbs sampler for a simple UC model using simulation smoother\n############################################################\n\nUC.Gibbs.sampler    = function(S, starting.values, priors){\n  # Initialise the data \n  aux     = starting.values\n  T       = nrow(aux$Y)\n  i_matrix &lt;- diag(T)\n  i &lt;- matrix(0, T, 1)  \n  i[1, 1] &lt;- 1 \n  # Posteriors list\n  posteriors    = list(\n    tau     = matrix(NA,T,S),\n    tau_0   = matrix(NA,1,S),\n    sigma   = matrix(NA,2,S),\n    non.stationary.iterations = rep(NA,S)\n  )\n  \n  for (s in 1:S){\n    \n    # Sampling tau_0\n    ###########################\n    tau_0.v.inv    = diag(1/diag(priors$tau_0.v))\n    V.tau_0.bar    = solve((1/aux$sigma[1])*i_matrix + tau_0.v.inv )\n    tau_0.bar      = V.tau_0.bar %*% ( (1/aux$sigma[1])*i_matrix%*%priors$H%*%aux$tau%*%i + tau_0.v.inv%*%priors$tau_0 )\n    tau_0.draw     = rmvnorm(1,as.vector(tau_0.bar),V.tau_0.bar)\n    aux$tau_0      = as.vector(tau_0.draw)\n    \n    # Sampling sigma\n    ###########################\n    # sigma of tau (sigma_eta)\n    sigma.eta.s   = as.numeric(priors$sigma.s + crossprod(priors$H%*%aux$tau - i%*%priors$tau_0))\n    sigma.eta.nu  = priors$sigma.nu + T\n    sigma.eta.draw = sigma.eta.s/rchisq(1,sigma.eta.nu)\n    # sigma of errors (sigma)\n    sigma.e.s     = as.numeric(priors$sigma.s + crossprod(aux$Y - aux$tau))\n    sigma.e.nu    = priors$sigma.nu + T\n    sigma.e.draw  = sigma.e.s/rchisq(1,sigma.e.nu)\n    aux$sigma     = c(sigma.eta.draw,sigma.e.draw)\n    \n    # Sampling tau\n    ###########################\n    V.tau.inv     = (1/aux$sigma[2])*i_matrix + (1/aux$sigma[1])*crossprod(priors$H)\n    V.tau.inv     = 0.5*(V.tau.inv + t(V.tau.inv))\n    b.tau         = (1/aux$sigma[2])*i_matrix%*%aux$Y + (1/aux$sigma[1])%*%i_matrix%*%t(priors$H)%*%i%*%priors$tau_0\n    precision.L   = t(bandchol(V.tau.inv))\n    epsilon       = rnorm(T)\n    b.tau.tmp     = forwardsolve(precision.L, b.tau)\n    tau.draw      = backsolve(t(precision.L), b.tau.tmp + epsilon)\n    aux$tau       = tau.draw\n    \n    posteriors$tau[,s]     = aux$tau\n    posteriors$tau_0[,s]   = aux$tau_0\n    posteriors$sigma[,s]   = aux$sigma\n    # posteriors$non.stationary.iterations[s] = ns.i\n    if (s%%1000==0){cat(\" \",s)}\n  }\n  \n  output      = list(\n    posterior = posteriors,\n    last.draw = aux\n  )\n  return(output)\n}"
  },
  {
    "objectID": "index.html#simulation-smoother-and-precision-sampler",
    "href": "index.html#simulation-smoother-and-precision-sampler",
    "title": "Bayesian Unobserved Component Models",
    "section": "Simulation smoother and precision sampler",
    "text": "Simulation smoother and precision sampler"
  },
  {
    "objectID": "index.html#analytical-solution-for-a-joint-posterior",
    "href": "index.html#analytical-solution-for-a-joint-posterior",
    "title": "Bayesian Unobserved Component Models",
    "section": "Analytical solution for a joint posterior",
    "text": "Analytical solution for a joint posterior"
  },
  {
    "objectID": "index.html#estimating-gamma-error-term-variance-prior-scale",
    "href": "index.html#estimating-gamma-error-term-variance-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating gamma error term variance prior scale",
    "text": "Estimating gamma error term variance prior scale"
  },
  {
    "objectID": "index.html#estimating-inverted-gamma-2-error-term-variance-prior-scale",
    "href": "index.html#estimating-inverted-gamma-2-error-term-variance-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating inverted-gamma 2 error term variance prior scale",
    "text": "Estimating inverted-gamma 2 error term variance prior scale"
  },
  {
    "objectID": "index.html#estimating-the-initial-condition-prior-scale",
    "href": "index.html#estimating-the-initial-condition-prior-scale",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating the initial condition prior scale",
    "text": "Estimating the initial condition prior scale"
  },
  {
    "objectID": "index.html#student-t-prior-for-the-trend-component",
    "href": "index.html#student-t-prior-for-the-trend-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Student-t prior for the trend component",
    "text": "Student-t prior for the trend component"
  },
  {
    "objectID": "index.html#estimating-student-t-degrees-of-freedom-parameter",
    "href": "index.html#estimating-student-t-degrees-of-freedom-parameter",
    "title": "Bayesian Unobserved Component Models",
    "section": "Estimating Student-t degrees of freedom parameter",
    "text": "Estimating Student-t degrees of freedom parameter\nThe Student-t distribution is commonly used in statistical modeling to handle data with heavier tails than the normal distribution. An essential parameter of the Student-t distribution is the degrees of freedom \\(\\nu\\), which controls the tail heaviness. In this note, we present the Bayesian estimation of the degrees of freedom parameter for an N-variate Student-t distribution using the Inverted-Gamma 2 (IG2) scale mixture of normals.\nThe N-variate Student-t distribution can be represented as a scale mixture of normals:\n\\[\n\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda \\sim \\mathcal{N}(\\mathbf{\\mu}, \\lambda \\mathbf{I}_N)\n\\]\n\\[\n\\lambda \\mid \\nu \\sim \\mathcal{IG2}(\\nu, \\nu)\n\\]\nwhere:\n\n\\(\\mathbf{y}\\) is the \\(N\\)-dimensional observation vector.\n\\(\\mathbf{\\mu}\\) is the mean vector.\n\\(\\lambda\\) is the latent scale variable.\n\\(\\nu\\) is the degrees of freedom parameter.\n\n\nDerivation of Full Conditional Posteriors\n\nFull Conditional Posterior of \\(\\lambda\\)\nGiven the prior distribution:\n\\[\n\\lambda \\mid \\nu \\sim \\mathcal{IG2}(\\nu, \\nu)\n\\]\nThe likelihood of the data given \\(\\lambda\\) is:\n\\[\n\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda \\sim \\mathcal{N}(\\mathbf{\\mu}, \\lambda \\mathbf{I}_N)\n\\]\nThe full conditional posterior of \\(\\lambda\\) can be derived as follows:\n\nLikelihood of \\(\\mathbf{y}\\) given \\(\\mathbf{\\mu}\\) and \\(\\lambda\\):\n\\[\np(\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda) \\propto \\lambda^{-\\frac{N}{2}} \\exp\\left(-\\frac{(\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu})}{2\\lambda}\\right)\n\\]\nPrior for \\(\\lambda\\) given \\(\\nu\\):\n\\[\np(\\lambda \\mid \\nu) \\propto \\lambda^{-\\nu - 1} \\exp\\left(-\\frac{\\nu}{\\lambda}\\right)\n\\]\nJoint distribution:\n\\[\np(\\mathbf{y}, \\lambda \\mid \\mathbf{\\mu}, \\nu) = p(\\mathbf{y} \\mid \\mathbf{\\mu}, \\lambda) p(\\lambda \\mid \\nu)\n\\]\nFull conditional posterior:\n\\[\np(\\lambda \\mid \\mathbf{y}, \\mathbf{\\mu}, \\nu) \\propto \\lambda^{-\\frac{N}{2}} \\exp\\left(-\\frac{(\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu})}{2\\lambda}\\right) \\lambda^{-\\nu - 1} \\exp\\left(-\\frac{\\nu}{\\lambda}\\right)\n\\]\nCombining terms:\n\\[\np(\\lambda \\mid \\mathbf{y}, \\mathbf{\\mu}, \\nu) \\propto \\lambda^{-\\left(\\nu + \\frac{N}{2} + 1\\right)} \\exp\\left(-\\frac{\\nu + \\frac{(\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu})}{2}}{\\lambda}\\right)\n\\]\nThis is recognized as the kernel of an Inverted-Gamma 2 distribution:\n\\[\n\\lambda \\mid \\mathbf{y}, \\mathbf{\\mu}, \\nu \\sim \\mathcal{IG2}\\left(\\nu + N, \\nu + (\\mathbf{y} - \\mathbf{\\mu})'(\\mathbf{y} - \\mathbf{\\mu})\\right)\n\\]\n\n\n\nFull Conditional Posterior of \\(\\nu\\)\nTo estimate \\(\\nu\\), we use the Metropolis-Hastings algorithm due to its non-standard form. The steps for deriving the full conditional posterior of \\(\\nu\\) are as follows:\n\nLikelihood of \\(\\lambda\\) given \\(\\nu\\):\n\\[\np(\\lambda \\mid \\nu) = \\frac{\\left(\\frac{\\nu}{2}\\right)^{\\nu/2}}{\\Gamma(\\nu/2)} \\lambda^{-\\left(\\nu/2 + 1\\right)} \\exp\\left(-\\frac{\\nu}{2\\lambda}\\right)\n\\]\nLog-likelihood for \\(\\nu\\) given \\(\\lambda\\):\n\\[\n\\log p(\\lambda \\mid \\nu) = \\frac{\\nu}{2} \\log\\left(\\frac{\\nu}{2}\\right) - \\log\\Gamma\\left(\\frac{\\nu}{2}\\right) - \\left(\\frac{\\nu}{2} + 1\\right) \\log \\lambda - \\frac{\\nu}{2\\lambda}\n\\]\nLog-prior for \\(\\nu\\) (assuming a non-informative prior):\n\\[\n\\log p(\\nu) = \\text{constant}\n\\]\nFull conditional posterior:\nThe full conditional posterior for \\(\\nu\\) is proportional to the product of the likelihood and the prior:\n\\[\np(\\nu \\mid \\lambda) \\propto p(\\lambda \\mid \\nu) p(\\nu)\n\\]\nSince \\(p(\\nu)\\) is constant, we focus on \\(p(\\lambda \\mid \\nu)\\):\n\\[\n\\log p(\\nu \\mid \\lambda) = \\frac{\\nu}{2} \\log\\left(\\frac{\\nu}{2}\\right) - \\log\\Gamma\\left(\\frac{\\nu}{2}\\right) - \\left(\\frac{\\nu}{2} + 1\\right) \\log \\lambda - \\frac{\\nu}{2\\lambda}\n\\]\nThis expression does not have a closed form, so we use the Metropolis-Hastings algorithm to sample from this posterior.\n\n\n\n\nR Function for Gibbs Sampler\nBelow is the R function implementing the Gibbs sampler for estimating \\(\\nu\\) using the IG2-scale mixture of normals representation.\n\nmetropolis_hastings_nu &lt;- function(y, mu, n_iter, init_nu, proposal_sd) {\n  # Initialize parameter\n  nu &lt;- init_nu\n  N &lt;- length(y)\n  \n  # Storage for samples\n  nu_samples &lt;- numeric(n_iter)\n  \n  # Log-likelihood function\n  log_likelihood &lt;- function(nu, y, mu) {\n    sum(dt((y - mu) / sqrt(nu), df = nu, log = TRUE))\n  }\n  \n  for (i in 1:n_iter) {\n    # Propose new value for nu\n    nu_proposal &lt;- nu + rnorm(1, 0, proposal_sd)\n    \n    if (nu_proposal &gt; 0) {\n      # Calculate log acceptance ratio\n      log_acceptance_ratio &lt;- log_likelihood(nu_proposal, y, mu) - log_likelihood(nu, y, mu)\n      \n      # Accept or reject the proposal\n      if (log(runif(1)) &lt; log_acceptance_ratio) {\n        nu &lt;- nu_proposal\n      }\n    }\n    \n    # Store the sample\n    nu_samples[i] &lt;- nu\n  }\n  \n  return(nu_samples)\n}\n\ngibbs_sampler_t &lt;- function(y, n_iter, init_values) {\n  # Initialize parameters\n  nu &lt;- init_values$nu\n  mu &lt;- init_values$mu\n  N &lt;- length(y)\n  \n  # Storage for samples\n  nu_samples &lt;- numeric(n_iter)\n  mu_samples &lt;- numeric(n_iter)\n  lambda_samples &lt;- numeric(n_iter)\n  \n  for (i in 1:n_iter) {\n    # Sample lambda\n    shape_lambda &lt;- nu + N\n    rate_lambda &lt;- nu + sum((y - mu)^2)\n    lambda &lt;- 1 / rgamma(1, shape = shape_lambda, rate = rate_lambda)\n    \n    # Sample mu\n    mu &lt;- rnorm(1, mean = mean(y), sd = sqrt(lambda / N))\n    \n    # Sample nu using Metropolis-Hastings\n    log_likelihood &lt;- function(nu, y, mu) {\n      sum(dt((y - mu) / sqrt(nu), df = nu, log = TRUE))\n    }\n    \n    proposal_nu &lt;- nu + rnorm(1, 0, 0.1) # proposal distribution: normal random walk\n    if (proposal_nu &gt; 0) {\n      log_acceptance_ratio &lt;- log_likelihood(proposal_nu, y, mu) - log_likelihood(nu, y, mu)\n      if (log(runif(1)) &lt; log_acceptance_ratio\n\n) {\n        nu &lt;- proposal_nu\n      }\n    }\n    \n    # Store samples\n    nu_samples[i] &lt;- nu\n    mu_samples[i] &lt;- mu\n    lambda_samples[i] &lt;- lambda\n  }\n  \n  return(list(nu = nu_samples, mu = mu_samples, lambda = lambda_samples))\n}\n\n# Example usage\nset.seed(123)\ny &lt;- rnorm(100)\ninit_values &lt;- list(nu = 5, mu = mean(y))\nn_iter &lt;- 1000\nresult &lt;- gibbs_sampler_t(y, n_iter, init_values)\n\n# Display the results\nprint(summary(result$nu))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  5.118   6.414   6.947   7.398   7.888  10.821 \n\nprint(summary(result$mu))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.19910  0.02664  0.08867  0.08544  0.14827  0.34116 \n\nprint(summary(result$lambda))\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6030  0.7907  0.8465  0.8537  0.9095  1.1743 \n\n\n\n\nConclusion\nThis note provided a comprehensive step-by-step algebraic derivation and a sampler for estimating the degrees of freedom parameter \\(\\nu\\) for an N-variate Student-t distribution using an IG2-scale mixture of normals approach within a Bayesian framework. By using the Metropolis-Hastings algorithm, we avoid the need to assume a prior distribution for \\(\\nu\\), simplifying the estimation process. This approach allows for flexible modeling of heavy-tailed data, which is often encountered in practice.\n\n\nReferences\n\nGeweke, J. (1993). Bayesian treatment of the independent Student-t linear model. Journal of Applied Econometrics, 8(S1), S19-S40.\nChib, S., & Greenberg, E. (1995). Understanding the Metropolis-Hastings Algorithm. The American Statistician, 49(4), 327-335."
  },
  {
    "objectID": "index.html#laplace-prior-for-the-trend-component",
    "href": "index.html#laplace-prior-for-the-trend-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Laplace prior for the trend component",
    "text": "Laplace prior for the trend component"
  },
  {
    "objectID": "index.html#autoregressive-cycle-component",
    "href": "index.html#autoregressive-cycle-component",
    "title": "Bayesian Unobserved Component Models",
    "section": "Autoregressive cycle component",
    "text": "Autoregressive cycle component"
  },
  {
    "objectID": "index.html#random-walk-with-time-varying-drift-parameter",
    "href": "index.html#random-walk-with-time-varying-drift-parameter",
    "title": "Bayesian Unobserved Component Models",
    "section": "Random walk with time-varying drift parameter",
    "text": "Random walk with time-varying drift parameter"
  },
  {
    "objectID": "index.html#student-t-error-terms",
    "href": "index.html#student-t-error-terms",
    "title": "Bayesian Unobserved Component Models",
    "section": "Student-t error terms",
    "text": "Student-t error terms"
  },
  {
    "objectID": "index.html#conditional-heteroskedasticity",
    "href": "index.html#conditional-heteroskedasticity",
    "title": "Bayesian Unobserved Component Models",
    "section": "Conditional heteroskedasticity",
    "text": "Conditional heteroskedasticity"
  },
  {
    "objectID": "index.html#predictive-density",
    "href": "index.html#predictive-density",
    "title": "Bayesian Unobserved Component Models",
    "section": "Predictive density",
    "text": "Predictive density"
  },
  {
    "objectID": "index.html#sampling-from-the-predictive-density",
    "href": "index.html#sampling-from-the-predictive-density",
    "title": "Bayesian Unobserved Component Models",
    "section": "Sampling from the predictive density",
    "text": "Sampling from the predictive density"
  },
  {
    "objectID": "index.html#missing-observations",
    "href": "index.html#missing-observations",
    "title": "Bayesian Unobserved Component Models",
    "section": "Missing observations",
    "text": "Missing observations"
  },
  {
    "objectID": "index.html#references-1",
    "href": "index.html#references-1",
    "title": "Bayesian Unobserved Component Models",
    "section": "References",
    "text": "References"
  }
]